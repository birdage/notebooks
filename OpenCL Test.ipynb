{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyopencl as cl\n",
      "import numpy\n",
      "import numpy.linalg as la\n",
      "\n",
      "a = numpy.random.rand(50000).astype(numpy.float32)\n",
      "b = numpy.random.rand(50000).astype(numpy.float32)\n",
      "\n",
      "ctx = cl.create_some_context()\n",
      "queue = cl.CommandQueue(ctx)\n",
      "\n",
      "mf = cl.mem_flags\n",
      "a_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a)\n",
      "b_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b)\n",
      "dest_buf = cl.Buffer(ctx, mf.WRITE_ONLY, b.nbytes)\n",
      "\n",
      "prg = cl.Program(ctx, \"\"\"\n",
      "    __kernel void sum(__global const float *a,\n",
      "    __global const float *b, __global float *c)\n",
      "    {\n",
      "      int gid = get_global_id(0);\n",
      "      c[gid] = a[gid] + b[gid];\n",
      "    }\n",
      "    \"\"\").build()\n",
      "\n",
      "prg.sum(queue, a.shape, None, a_buf, b_buf, dest_buf)\n",
      "\n",
      "a_plus_b = numpy.empty_like(a)\n",
      "cl.enqueue_copy(queue, a_plus_b, dest_buf)\n",
      "\n",
      "print(la.norm(a_plus_b - (a+b)), la.norm(a_plus_b))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.0, 241.89145)\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyopencl as cl\n",
      "from pyopencl import array\n",
      "import numpy as np\n",
      "\n",
      "vector = np.zeros((1,1), cl.array.vec.float4)\n",
      "matrix = np.zeros((1,4), cl.array.vec.float4)\n",
      "matrix[0,0] = (1, 2, 4, 8)\n",
      "matrix[0,1] = (16, 32, 64, 128)\n",
      "matrix[0,2] = (3, 6, 8, 12)\n",
      "matrix[0,0] = (5, 10, 15, 25)\n",
      "vector[0,0] = (1, 2, 4, 8)\n",
      "\n",
      "# Step 1. Get the OpenCL platform\n",
      "platform = cl.get_platforms()[0]\n",
      "\n",
      "# Step 2 Obtain a device id for at least one device\n",
      "device = platform.get_devices()[1]\n",
      "print 'Available Devices on', platform.name, ':'\n",
      "for dev in platform.get_devices():\n",
      "    print ' -', dev.name\n",
      "# Step 3. Create a context for the device\n",
      "context = cl.Context([device])\n",
      "\n",
      "# Step 4. Create the accelerator program\n",
      "# Step 5. Build the program\n",
      "# Step 6. Create one or more kernels from the program functions\n",
      "\n",
      "program = cl.Program(context, '''\n",
      "__kernel void matrix_dot_vector(__global const float4 *matrix,\n",
      "                                __global const float4 *vector,\n",
      "                                __global float *result) {\n",
      "    int gid = get_global_id(0);\n",
      "    result[gid] = dot(matrix[gid], vector[0]);\n",
      "}\n",
      "''')\n",
      "program.build()\n",
      "\n",
      "# Step 7. Create a command queue for the target device\n",
      "queue = cl.CommandQueue(context)\n",
      "\n",
      "# Step 8. Allocate device memory and move input data from the host\n",
      "mem_flags = cl.mem_flags\n",
      "matrix_buf = cl.Buffer(context, mem_flags.READ_ONLY | mem_flags.COPY_HOST_PTR, hostbuf=matrix)\n",
      "vector_buf = cl.Buffer(context, mem_flags.READ_ONLY | mem_flags.COPY_HOST_PTR, hostbuf=vector)\n",
      "matrix_dot_vector = np.zeros(4, np.float32)\n",
      "destination_buf = cl.Buffer(context, mem_flags.WRITE_ONLY, matrix_dot_vector.nbytes)\n",
      "\n",
      "# Step 9. Associate the arguments to the kernel with kernel object.\n",
      "# Step 10. Deploy the kernel for device execution.\n",
      "program.matrix_dot_vector(queue, matrix_dot_vector.shape, None, matrix_buf, vector_buf, destination_buf)\n",
      "\n",
      "# Step 11. Move the kernel's output data to host memory.\n",
      "cl.enqueue_copy(queue, matrix_dot_vector, destination_buf)\n",
      "\n",
      "# Step 12. Release context, program, kernels, and memory.\n",
      "print matrix_dot_vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Available Devices on Apple :\n",
        " - Intel(R) Core(TM) i7-2720QM CPU @ 2.20GHz\n",
        " - ATI Radeon HD 6750M\n",
        "[  285.  1360.   143.     0.]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyopencl as cl\n",
      "import numpy as np\n",
      "import time\n",
      "\n",
      "\n",
      "kernel_code = \"\"\"\n",
      "\n",
      "#define TIMESTEPS   %d\n",
      "\n",
      "    float u_func(float x, float y, float t);\n",
      "    float v_func(float x, float y, float t);\n",
      "    float u_func(float x, float y, float t) {\n",
      "        float G = 0.01f;\n",
      "        float Q = 0.000002f;\n",
      "        float Rs = x*x + y*y;\n",
      "        float u = (G/2 * M_PI) * y * pow(10, -1 * (Q * Rs));\n",
      "        return u;\n",
      "    }\n",
      "    float v_func(float x, float y, float t) {\n",
      "        float G = 0.01f;\n",
      "        float Q = 0.000002f;\n",
      "        float Rs = x*x + y*y;\n",
      "        float v = (G/2 * M_PI) * x * pow(10, -1 * (Q * Rs));\n",
      "        return v;\n",
      "    }\n",
      "    __kernel void euler(__global const float *x,\n",
      "                        __global const float *y,\n",
      "                        __global float *xp,\n",
      "                        __global float *yp)\n",
      "    {\n",
      "      int i = get_global_id(0);\n",
      "      xp[i] = x[i];\n",
      "      yp[i] = y[i];\n",
      "      for(int t=0; t < TIMESTEPS; t++) {\n",
      "          float u = u_func(xp[i], yp[i], i);\n",
      "          float v = v_func(xp[i], yp[i], i);\n",
      "          xp[i] = xp[i] + u;\n",
      "          yp[i] = yp[i] + v;\n",
      "      }\n",
      "    }\n",
      "    \n",
      "    \n",
      "\"\"\"\n",
      "\n",
      "platform = cl.get_platforms()[0]\n",
      "\n",
      "def run_this_shit(device_id, particle_count, spread, timesteps):\n",
      "\n",
      "    h_x = np.random.rand(particle_count).astype(np.float32) * spread - (spread/2)\n",
      "    h_y = np.random.rand(particle_count).astype(np.float32) * spread - (spread/2)\n",
      "\n",
      "\n",
      "    # device[0] = CPU\n",
      "    # device[1] = GPU\n",
      "    device = platform.get_devices()[device_id] # Get the GPU not the CPU\n",
      "\n",
      "    ctx = cl.Context([device])\n",
      "\n",
      "    mf = cl.mem_flags\n",
      "    # Copy host memory from h_x and h_y to device buffers d_x and d_y\n",
      "    d_x = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_x)\n",
      "    d_y = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_y)\n",
      "\n",
      "    # Make two buffers for our output xp and yp\n",
      "    d_xp = cl.Buffer(ctx, mf.WRITE_ONLY, h_x.nbytes)\n",
      "    d_yp = cl.Buffer(ctx, mf.WRITE_ONLY, h_y.nbytes)\n",
      "\n",
      "    queue = cl.CommandQueue(ctx)\n",
      "\n",
      "    prg = cl.Program(ctx, kernel_code % timesteps).build()\n",
      "    #simple = cl.Program(ctx, simple_kernel).build()\n",
      "    times = []\n",
      "    for i in xrange(10):\n",
      "        t_then = time.time()\n",
      "        event = prg.euler(queue, h_x.shape, None, d_x, d_y, d_xp, d_yp)\n",
      "        event.wait()\n",
      "        times.append(time.time() - t_then)\n",
      "\n",
      "    print ','.join(['%s' % i for i in (device_id, np.log10(particle_count), timesteps, np.mean(times))])\n",
      "\n",
      "\n",
      "    #simple.simple(queue, h_x.shape, None, d_x, d_xp)\n",
      "\n",
      "    h_xp = np.zeros(h_x.shape, dtype=np.float32)\n",
      "    h_yp = np.zeros(h_y.shape, dtype=np.float32)\n",
      "    # Copy the values from the device buffer d_xp to h_xp, the host \n",
      "    cl.enqueue_copy(queue, h_xp, d_xp)\n",
      "    cl.enqueue_copy(queue, h_yp, d_yp)\n",
      "\n",
      "    #plt.plot(h_x, h_y, 'bo')\n",
      "    #plt.plot(h_xp, h_yp, 'ro')\n",
      "\n",
      "\n",
      "device_id = 0\n",
      "device = platform.get_devices()[device_id]\n",
      "print 'running on',device.name\n",
      "print\"device,particle_count,timesteps,mean\"\n",
      "t_now = time.time()\n",
      "for particle_exp in xrange(5):\n",
      "    for timestep_i in xrange(5):\n",
      "        run_this_shit(device_id, 10 ** particle_exp, 100, timestep_i * 1000)\n",
      "\n",
      "print 'Total Time:', (time.time() - t_now)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LogicError",
       "evalue": "clEnqueueNDRangeKernel failed: invalid work group size",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-fe6245820cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparticle_exp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtimestep_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mrun_this_shit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mparticle_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Total Time:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-7-fe6245820cde>\u001b[0m in \u001b[0;36mrun_this_shit\u001b[0;34m(device_id, particle_count, spread, timesteps)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mt_then\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meuler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_xp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_yp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_then\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/luke/Documents/Dev/virtenvs/work/lib/python2.7/site-packages/pyopencl/__init__.pyc\u001b[0m in \u001b[0;36mkernel_call\u001b[0;34m(self, queue, global_size, local_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         return enqueue_nd_range_kernel(queue, self, global_size, local_size,\n\u001b[0;32m--> 466\u001b[0;31m                 global_offset, wait_for, g_times_l=g_times_l)\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkernel_set_scalar_arg_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mLogicError\u001b[0m: clEnqueueNDRangeKernel failed: invalid work group size"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "running on Intel(R) Core(TM) i7-2640M CPU @ 2.80GHz\n",
        "device,particle_count,timesteps,mean\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}